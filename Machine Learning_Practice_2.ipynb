{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specialized-impossible",
   "metadata": {},
   "source": [
    "### I didn't see the additional optional assignment of Lab 8 so I have practiced PySpark Machine Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-graph",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "synthetic-brave",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T00:29:31.155834Z",
     "start_time": "2024-02-02T00:29:28.895919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark mllib\")\\\n",
    "    .master('local[*]')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-penalty",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using PCA From Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adjusted-validation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T00:34:51.683655Z",
     "start_time": "2024-02-02T00:34:50.381554Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-qatar",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-bedroom",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-copyright",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using Linear regression From Classification and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pleasant-finland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T00:43:03.760956Z",
     "start_time": "2024-02-02T00:43:00.591253Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.32292516677405936,-0.3438548034562218,1.9156017023458414,0.05288058680386263,0.765962720459771,0.0,-0.15105392669186682,-0.21587930360904642,0.22025369188813426]\n",
      "Intercept: 0.1598936844239736\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-instrumentation",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using Train-Validation Split From model selection and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accessible-champagne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T00:47:56.511171Z",
     "start_time": "2024-02-02T00:47:48.942403Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...| -17.026492264209548| -1.7800622423486911|\n",
      "|(10,[0,1,2,3,4,5,...|  -16.71909683360509| -0.1893325701092588|\n",
      "|(10,[0,1,2,3,4,5,...| -15.375857723312297|  0.7252323736487188|\n",
      "|(10,[0,1,2,3,4,5,...| -13.772441561702871|  3.2696413241677718|\n",
      "|(10,[0,1,2,3,4,5,...| -13.039928064104615| 0.18817684046065764|\n",
      "|(10,[0,1,2,3,4,5,...|   -9.42898793151394|  -3.449987079269568|\n",
      "|(10,[0,1,2,3,4,5,...|    -9.2679651250406| -0.3310907549069632|\n",
      "|(10,[0,1,2,3,4,5,...|  -9.173693798406978|-0.42727135281551937|\n",
      "|(10,[0,1,2,3,4,5,...| -7.1500991588127265|   2.936884251408867|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.930603551528371|-0.02839768193150...|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.456944198081549| -0.9224776887934015|\n",
      "|(10,[0,1,2,3,4,5,...| -3.2843694575334834| -1.0821208483033875|\n",
      "|(10,[0,1,2,3,4,5,...|   -1.99891354174786|  0.8052068273813595|\n",
      "|(10,[0,1,2,3,4,5,...| -0.4683784136986876|  0.5046267770459569|\n",
      "|(10,[0,1,2,3,4,5,...|-0.44652227528840105| 0.05307214502058932|\n",
      "|(10,[0,1,2,3,4,5,...| 0.10157453780074743| -1.0931313634366817|\n",
      "|(10,[0,1,2,3,4,5,...|  0.2105613019270259|  1.0684044436721878|\n",
      "|(10,[0,1,2,3,4,5,...|  2.1214592666251364| 0.10226191630980651|\n",
      "|(10,[0,1,2,3,4,5,...|  2.8497179990245116|  1.1908709522287462|\n",
      "|(10,[0,1,2,3,4,5,...|   3.980473021620311|  2.3611397922073025|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-photographer",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

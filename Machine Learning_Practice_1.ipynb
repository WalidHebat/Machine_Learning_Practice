{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stone-appliance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T12:42:51.246123Z",
     "start_time": "2024-01-29T12:42:50.934148Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "removed-egyptian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T12:42:58.348701Z",
     "start_time": "2024-01-29T12:42:51.269864Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark mllib\")\\\n",
    "    .master('local[*]')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diagnostic-english",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:23:57.789951Z",
     "start_time": "2024-01-29T14:23:57.776268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://99356280c1c7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark mllib</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f419caae5e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-focus",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "difficult-tribune",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T13:34:13.957085Z",
     "start_time": "2024-01-29T13:34:13.249832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thousand-platinum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T13:34:18.403950Z",
     "start_time": "2024-01-29T13:34:16.908236Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+--------------------+\n",
      "| id|            text|label|               words|\n",
      "+---+----------------+-----+--------------------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|\n",
      "|  1|             b d|  0.0|              [b, d]|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]|\n",
      "+---+----------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.transform(training).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "minimal-direction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:25:26.062728Z",
     "start_time": "2024-01-29T14:25:22.236618Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.15964077387874118,0.8403592261212589], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.8378325685476612,0.16216743145233875], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976273,0.9307336686702373], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444208,0.01784246665557917], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction_df = model.transform(test)\n",
    "selected = prediction_df.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cutting-tribe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:25:32.269753Z",
     "start_time": "2024-01-29T14:25:32.263145Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "civic-spank",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T21:02:56.667503Z",
     "start_time": "2024-01-27T21:02:56.359316Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[19036,68...|[-1.6609033227473...|[0.15964077387874...|       1.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[1303,526...|[1.64218895265635...|[0.83783256854766...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[173558,1...|[-2.5980142174392...|[0.06926633132976...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[68303,19...|[4.00817033336806...|[0.98215753334442...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-brunei",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-transcript",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "optical-fluid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T22:00:29.499328Z",
     "start_time": "2024-01-27T22:00:29.076554Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.0026943832635879517,-0.0496953159570694,-0.05644454658031464,0.05291593065485359]\n",
      "\n",
      "Text: [Hey, I, heard, what, is, Spark] => \n",
      "Vector: [0.024073704281666625,-0.0386797961158057,-0.07569998564819494,0.020811142011856038]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.061852710853729925,-0.0027706625738314217,-0.009995942324167117,0.0008023796336991446]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.05253076292574406,-0.001811251137405634,-0.060513718798756604,-0.06604135092347861]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"Hey I heard what is Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=4, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-founder",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "automated-climb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T19:52:33.434515Z",
     "start_time": "2024-01-27T19:52:32.875871Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-sweet",
   "metadata": {},
   "source": [
    "# Feature Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-victor",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reflected-council",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-28T16:11:51.259749Z",
     "start_time": "2024-01-28T16:11:50.885229Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,Models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,Models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "c=countTokens(col(\"words\"))\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", c).show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complex-buffer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-28T16:09:10.918251Z",
     "start_time": "2024-01-28T16:09:10.911817Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grand-wildlife",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-28T16:07:51.155233Z",
     "start_time": "2024-01-28T16:07:51.134927Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legitimate-samuel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:47.009985Z",
     "start_time": "2024-01-28T16:20:46.695271Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,Models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W+\")\n",
    "\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-trial",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "starting-courtesy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-28T16:22:45.479235Z",
     "start_time": "2024-01-28T16:22:44.930358Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-vacuum",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "correct-trailer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T13:14:06.621437Z",
     "start_time": "2024-01-29T13:13:59.963723Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "dataFrame = spark.read.format(\"libsvm\").load(\"Data/sample_libsvm_data.txt\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-nursing",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "approximate-token",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T14:42:52.323550Z",
     "start_time": "2024-01-29T14:42:51.446361Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-answer",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "technological-episode",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T19:48:58.409587Z",
     "start_time": "2024-01-27T19:48:57.975531Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 1.0, 3.0),\n",
    "    (2, 2.0, 5.0)\n",
    "], [\"id\", \"v1\", \"v2\"])\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "sqlTrans.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-iraqi",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-voltage",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatty-sullivan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:07:46.358386Z",
     "start_time": "2024-01-29T15:07:46.131317Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"Data/sample_libsvm_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "atmospheric-motion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:02:37.108877Z",
     "start_time": "2024-01-27T20:02:36.581976Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "leading-swing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:07:50.288245Z",
     "start_time": "2024-01-29T15:07:49.574597Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dynamic-saskatchewan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:07:52.474918Z",
     "start_time": "2024-01-29T15:07:52.321650Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+\n",
      "|label|            features|indexedLabel|\n",
      "+-----+--------------------+------------+\n",
      "|  0.0|(692,[127,128,129...|         1.0|\n",
      "|  1.0|(692,[158,159,160...|         0.0|\n",
      "|  1.0|(692,[124,125,126...|         0.0|\n",
      "|  1.0|(692,[152,153,154...|         0.0|\n",
      "|  1.0|(692,[151,152,153...|         0.0|\n",
      "|  0.0|(692,[129,130,131...|         1.0|\n",
      "|  1.0|(692,[158,159,160...|         0.0|\n",
      "|  1.0|(692,[99,100,101,...|         0.0|\n",
      "|  0.0|(692,[154,155,156...|         1.0|\n",
      "|  0.0|(692,[127,128,129...|         1.0|\n",
      "|  1.0|(692,[154,155,156...|         0.0|\n",
      "|  0.0|(692,[153,154,155...|         1.0|\n",
      "|  0.0|(692,[151,152,153...|         1.0|\n",
      "|  1.0|(692,[129,130,131...|         0.0|\n",
      "|  0.0|(692,[154,155,156...|         1.0|\n",
      "|  1.0|(692,[150,151,152...|         0.0|\n",
      "|  0.0|(692,[124,125,126...|         1.0|\n",
      "|  0.0|(692,[152,153,154...|         1.0|\n",
      "|  1.0|(692,[97,98,99,12...|         0.0|\n",
      "|  1.0|(692,[124,125,126...|         0.0|\n",
      "+-----+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelIndexer.transform(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "distinct-proposal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:14:40.159697Z",
     "start_time": "2024-01-29T15:14:40.116776Z"
    }
   },
   "outputs": [],
   "source": [
    "df = featureIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "polar-forward",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:15:19.469043Z",
     "start_time": "2024-01-29T15:15:19.462210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "south-robin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T15:21:52.371106Z",
     "start_time": "2024-01-29T15:21:51.640792Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.json(\"Data/feature_indexer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fewer-martial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:06:34.010901Z",
     "start_time": "2024-01-27T20:06:27.658126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.04 \n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_80cc67ed094e, depth=2, numNodes=5, numClasses=2, numFeatures=692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-modem",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear Svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "premium-single",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T13:14:28.542691Z",
     "start_time": "2024-01-29T13:14:25.516418Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005170630317473439,-0.0001172288654973735,-8.882754836918948e-05,8.522360710187464e-05,0.0,0.0,-1.3436361263314267e-05,0.0003729569801338091,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0008888949552633658,0.00029864059761812683,0.0003793378816193159,-0.0001762328898254081,0.0,1.5028489269747836e-06,1.8056041144946687e-06,1.8028763260398597e-06,-3.3843713506473646e-06,-4.041580184807502e-06,2.0965017727015125e-06,8.536111642989494e-05,0.00022064177429604464,0.00021677599940575452,-0.0005472401396558763,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000921415502407147,0.00031351066886882195,0.0002481984318412822,0.0,-4.147738197636148e-05,-3.6832150384497175e-05,0.0,-3.9652366184583814e-06,-5.1569169804965594e-05,-6.624697287084958e-05,-2.182148650424713e-05,1.163442969067449e-05,-1.1535211416971104e-06,3.8138960488857075e-05,1.5823711634321492e-06,-4.784013432336632e-05,-9.386493224111833e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00043174897827077767,0.00017055492867397665,0.0,-2.7978204136148868e-05,-5.88745220385208e-05,-4.1858794529775e-05,-3.740692964881002e-05,-3.9787939304887e-05,-5.545881895011037e-05,-4.505015598421474e-05,-3.214002494749943e-06,-1.6561868808274739e-06,-4.416063987619447e-06,-7.9986183315327e-06,-4.729962112535003e-05,-2.516595625914463e-05,-3.6407809279248066e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00024719098130614967,0.0,-3.270637431382939e-05,-5.5703407875748054e-05,-5.2336892125702286e-05,-7.829604482365818e-05,-7.60385448387619e-05,-8.371051301348216e-05,-1.8669558753795108e-05,0.0,1.2045309486213725e-05,-2.3374084977016397e-05,-1.0788641688879534e-05,-5.5731194431606874e-05,-7.952979033591137e-05,-1.4529196775456057e-05,8.737948348132623e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0012589360772978808,-0.0001816228630214369,-0.00010650711664557365,-6.040355527710781e-05,-4.856392973921569e-05,-8.973895954652451e-05,-8.78131677062384e-05,-5.68487774673792e-05,-3.780926734276347e-05,1.3834897036553787e-05,7.585485129441565e-05,5.5017411816753975e-05,-1.5430755398169695e-05,-1.834928703625931e-05,-0.00010354008265646844,-0.00013527847721351194,-0.00011245007647684532,-2.9373916056750564e-05,-7.311217847336934e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0002858228613863785,-0.00012998173971449976,-0.0001478408021316135,-8.203374605865772e-05,-6.556685320008032e-05,-5.6392660386580244e-05,-6.995571627330911e-05,-4.664348159856693e-05,-2.3026593698824318e-05,7.398833979172035e-05,0.00014817176130099997,0.00010938317435545486,7.940425167011364e-05,-6.743294804348106e-07,-0.00012623302721464762,-0.00019110387355357616,-0.00018611622108961136,-0.00012776766254736952,-8.935302806524433e-05,-1.239417230441996e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0002829530831354112,-0.00013912189600461263,-0.00012593136464577562,-5.964745187930992e-05,-5.360328152341982e-05,-0.00010517880662090183,-0.00013856124131005022,-7.181032974125911e-05,2.3249038865093483e-06,0.0001566964269571967,0.00023261206954040812,0.00017261638232256968,0.00013857530960270466,-1.396299028868332e-05,-0.00015765773982418597,-0.00020728798812007546,-0.00019106441272002828,-0.00012744834161431415,-0.00012755611630280015,-5.1885591560478935e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000159081567023441,-0.0001216531230287931,-5.623851079809818e-05,-3.877987126382982e-05,-7.550900509956966e-05,-0.00010703140005463545,-0.00014720428138106226,-8.781423374509368e-05,7.941655609421792e-05,0.00023206354986219992,0.00027506982343672394,0.0002546722233188043,0.0001810821666388498,-1.3069916689929984e-05,-0.0001842374220886751,-0.0001977540482445517,-0.00017722074063670741,-0.0001487987014723575,-0.00011879021431288621,-9.755283887790393e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0001302740311359312,-5.3683030235535024e-05,-1.7631200013656873e-05,-7.846611034608254e-05,-0.000122100767283256,-0.00017281968533449702,-0.00015592346128894157,-5.239579492910452e-05,0.0001680719343542442,0.00028930086786548053,0.0003629921493231646,0.0002958223512266975,0.00021770466955449064,-6.40884808188951e-05,-0.00019058225556007997,-0.00020425138564600712,-0.0001711994903702119,-0.00013853486798341369,-0.00013018592950855062,-0.00011887779512760102,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.021411112285498e-05,-1.694500843168125e-05,-7.189722824172193e-05,-0.00014560828004346436,-0.00014935497340563198,-0.00019496419340776972,-0.00017383743417254187,-3.3438825792010694e-05,0.0002866538327947017,0.00029812321570739803,0.000377250607691119,0.0003211702827486386,0.0002577995115175486,-0.00016627385656703205,-0.00018037105851523224,-0.00020419356344211325,-0.00017962237203420184,-0.00013726488083579862,-0.00013461014473741762,-0.00012264216469164138,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0015239752514658556,-5.472330865993813e-05,-9.65684394936216e-05,-0.00013424729853486994,-0.00014727467799568,-0.0001616270978824712,-0.00018458259010029364,-0.00019699647135089726,0.00013085261294290817,0.0002943178857107149,0.0003097773692834126,0.0004112834769312103,0.00034113620757035025,0.00016529945924367265,-0.00021065410862650534,-0.0001883924081539624,-0.0001979586414569358,-0.0001762131187223702,-0.0001272343622678854,-0.00012708161719220297,-0.00014812221011889967,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.001140680600536578,-0.0001323467421269896,-0.00012904607854274846,-0.00014104748544921958,-0.00015194605434027872,-0.00021104539389774283,-0.00017911827582001795,-0.00018952948277194435,0.00021767571552539842,0.00030201791656326465,0.0004002863274397723,0.00040322806756364006,0.0004118077382608461,3.7917405252859545e-06,-0.00019886290660234838,-0.00019547443112937263,-0.00019857348218680872,-0.00013336892200703206,-0.00012830129292910815,-0.00011855916317355505,-0.0001765597203760205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010938769592297973,-0.00012785475305234688,-0.00013424699777466666,-0.0001505200652479287,-0.00019333287822872713,-0.00020385160086594937,-0.00017422470698847553,4.63598443910652e-05,0.00020617623087127652,0.0002862882891134514,0.0004074830988361515,0.0003726357785147985,0.0003507520190729629,-0.0001516485494364312,-0.00017053751921469217,-0.00019638964654350848,-0.00019962586265806435,-0.00013612312664311173,-0.0001218285533892454,-0.00011166712081624676,-0.0001377283888177579,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0003044386260118809,-0.0001240836643202059,-0.0001335317492716633,-0.00015783442604618277,-0.00019168434243384107,-0.00018710322733892716,-0.00011283989231463139,0.00011136504453105364,0.00018707244892705632,0.00028654279528966305,0.00040032117544983536,0.0003169637536305377,0.00020158994278679014,-0.00013139392844616033,-0.00015181070482383948,-0.0001825431845981843,-0.0001602539928567571,-0.00013230404795396355,-0.00011669138691257469,-0.00010532154964150405,-0.00013709037042366007,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00040287410145021705,-0.00013563987950912995,-0.00013225887084018914,-0.00016523502389794188,-0.00020175074284706945,-0.0001572459106394481,2.577536501278673e-06,0.0001312463663419457,0.00020707422291927531,0.00039081065544314936,0.00033487058329898135,0.00025790441367156086,2.6881819648016494e-05,-0.0001511383586714907,-0.0001605428139328567,-0.00017267287462873575,-0.00011938943768052963,-0.00010505245038633314,-0.00011109385509034013,-0.00013469914274864725,-0.00020735223736035555,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005034374233912422,-0.00015961213688405883,-0.0001274222123810994,-0.0001582821104884909,-0.00021301220616286252,-0.00012933366375029613,1.6802673102179614e-05,0.00011020918082727098,0.00021160795272688753,0.00034873421050827716,0.00026487211944380384,0.0001151606835026639,-5.4682731396851946e-05,-0.00013632001630934325,-0.00014340405857651405,-0.0001248695773821634,-8.462873247977974e-05,-9.580708414770257e-05,-0.00010749166605399431,-0.00014618038459197777,-0.00037556446296204636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005124342611878493,-0.00020369734099093433,-0.00013626985098328694,-0.00013313768183302705,-0.0001871555537819396,-0.0001188817315789655,-1.8774817595622694e-05,5.7108412194993384e-05,0.00012728161056121406,0.00019021458214915667,0.00012177397895874969,-1.2461153574281128e-05,-7.553961810487739e-05,-0.00010242174559410404,-4.44873554195981e-05,-9.058561577961895e-05,-6.837347198855518e-05,-8.084409304255458e-05,-0.00013316868299585082,-0.00020335916397646626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0003966510928472775,-0.00013738983629066386,-3.7971221409699866e-05,-6.431763035574533e-05,-0.00011857739882295322,-9.359520863114822e-05,-5.0878371516215046e-05,-8.269367595092908e-08,0.0,1.3434539131099211e-05,-1.9601690213728576e-06,-2.8527045990494954e-05,-7.410332699310603e-05,-7.132130570080122e-05,-4.9780961185536e-05,-6.641505361384578e-05,-6.962005514093816e-05,-7.752898158331023e-05,-0.00017393609499225025,-0.0012529479255443958,0.0,0.0,0.00020682521269893754,0.0,0.0,0.0,0.0,0.0,-0.00046702467383631055,-0.00010318036388792008,1.2004408785841247e-05,0.0,-2.5158639357650687e-05,-1.2095240910793449e-05,-5.19052816902203e-06,-4.916790639558058e-06,-8.48395853563783e-06,-9.362757097074547e-06,-2.0959335712838412e-05,-4.7790091043859085e-05,-7.92797600958695e-05,-4.462687041778011e-05,-4.182992428577707e-05,-3.7547996285851254e-05,-4.52754480225615e-05,-1.8553562561513456e-05,-0.00024763037962085644,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00034886180455242474,-5.687523659359091e-06,7.380040279654313e-05,4.395860636703821e-05,7.145198242379862e-05,6.181248343370637e-06,0.0,-6.0855538083486296e-05,-4.8563908323274725e-05,-4.117920588930435e-05,-4.359283623112936e-05,-6.608754161500044e-05,-5.443032251266018e-05,-2.7782637880987207e-05,0.0,0.0,0.0002879461393464088,-0.0028955529777851255,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00012312114837837392,-1.9526747917254753e-05,-1.6999506829961688e-05,5.4835294148085086e-05,1.523441632762399e-05,-5.8365604525328614e-05,-0.00012378194216521848,-0.00011750704953254656,-6.19711523061306e-05,-5.042009645812091e-05,-0.00014055260223565886,-0.0001410330942465528,-0.00019272308238929396,-0.0004802489964676616]\n",
      "Intercept: 0.012911305214513969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"Data/sample_libsvm_data.txt\")\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear SVC\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-alcohol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T19:54:30.535072Z",
     "start_time": "2024-01-27T19:54:30.531487Z"
    }
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-technology",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "public-keyboard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:16:22.208009Z",
     "start_time": "2024-01-27T20:16:21.238127Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.32292516677405936,-0.3438548034562218,1.9156017023458414,0.05288058680386263,0.765962720459771,0.0,-0.15105392669186682,-0.21587930360904642,0.22025369188813426]\n",
      "Intercept: 0.1598936844239736\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"Data/sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-macedonia",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "official-shelter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:17:57.026693Z",
     "start_time": "2024-01-27T20:17:55.127480Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|      0.05|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.03849\n",
      "RandomForestRegressionModel: uid=RandomForestRegressor_b251702fb8fe, numTrees=20, numFeatures=692\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"Data/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-height",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-guest",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "synthetic-minutes",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:19:09.867042Z",
     "start_time": "2024-01-27T20:19:07.355237Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"Data/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-cooking",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "micro-hospital",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:21:24.098168Z",
     "start_time": "2024-01-27T20:21:08.003385Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2661, 0.7339]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9209, 0.0791]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.4429, 0.5571]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.8584, 0.1416]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suited-foundation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T20:21:59.106370Z",
     "start_time": "2024-01-27T20:21:58.965368Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
